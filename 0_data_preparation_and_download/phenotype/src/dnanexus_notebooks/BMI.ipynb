{"metadata": {"language_info": {"name": "python", "version": "3.9.16", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}, "kernelspec": {"name": "python3", "display_name": "Python 3 (ipykernel)", "language": "python"}}, "nbformat_minor": 4, "nbformat": 4, "cells": [{"cell_type": "code", "source": "# Import packages\nimport pyspark\nimport dxpy\nimport dxdata", "metadata": {"trusted": true, "tags": []}, "execution_count": 1, "outputs": []}, {"cell_type": "code", "source": "# Spark initialization (Done only once; do not rerun this cell unless you select Kernel -> Restart kernel).\nsc = pyspark.SparkContext()\nspark = pyspark.sql.SparkSession(sc)", "metadata": {"trusted": true, "tags": [], "collapsed": true, "jupyter": {"outputs_hidden": true}}, "execution_count": 2, "outputs": [{"name": "stderr", "text": "SLF4J: Class path contains multiple SLF4J bindings.\nSLF4J: Found binding in [jar:file:/cluster/dnax/jars/dnanexus-api-0.1.0-SNAPSHOT-jar-with-dependencies.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: Found binding in [jar:file:/cluster/spark/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\nSLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n", "output_type": "stream"}, {"name": "stdout", "text": "2023-10-03 21:05:39.768 WARN  NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n2023-10-03 21:05:40.769 WARN  Utils:69 - Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 43000. Attempting port 43001.\n2023-10-03 21:05:41.002 WARN  MetricsReporter:84 - No metrics configured for reporting\n2023-10-03 21:05:41.004 WARN  LineProtoUsageReporter:48 - Telegraf configurations: url [metrics.push.telegraf.hostport], user [metrics.push.telegraf.user] or password [metrics.push.telegraf.password] missing.\n2023-10-03 21:05:41.004 WARN  MetricsReporter:117 - metrics.scraping.httpserver.port\n", "output_type": "stream"}]}, {"cell_type": "code", "source": "# Automatically discover dispensed database name and dataset id\ndispensed_database = dxpy.find_one_data_object(\n    classname='database', \n    name='app*', \n    folder='/', \n    name_mode='glob', \n    describe=True)\ndispensed_database_name = dispensed_database['describe']['name']\n\ndispensed_dataset = dxpy.find_one_data_object(\n    typename='Dataset', \n    name='app*.dataset', \n    folder='/', \n    name_mode='glob')\ndispensed_dataset_id = dispensed_dataset['id']", "metadata": {"trusted": true, "tags": []}, "execution_count": 3, "outputs": []}, {"cell_type": "code", "source": "dataset = dxdata.load_dataset(id=dispensed_dataset_id)", "metadata": {"trusted": true, "tags": []}, "execution_count": 4, "outputs": []}, {"cell_type": "code", "source": "participant = dataset['participant']", "metadata": {"trusted": true, "tags": []}, "execution_count": 5, "outputs": []}, {"cell_type": "code", "source": "field_name_dict = {\n    'sample_names': 'eid',\n    'sex': 'p31',\n    'bmi0': 'p21001_i0',\n    'bmi1': 'p21001_i1',\n    'bmi2': 'p21001_i2',\n    'genetic_kinship_to_other_participants': 'p22021',\n    'genetic_ethnic_grouping': 'p22006',\n    'genetic_sex': 'p22001',\n    'year_of_birth': 'p34',\n    'menopause0': 'p2724_i0',\n    'menopause1': 'p2724_i1',\n    'menopause2': 'p2724_i2',\n    'ethnic_background0': 'p21000_i0',\n    'ethnic_background1': 'p21000_i1',\n    'ethnic_background2': 'p21000_i2',\n    'bmi_prs': 'p26216',\n    'age_assessment0': 'p21003_i0',\n    'age_assessment1': 'p21003_i1',\n    'age_assessment2': 'p21003_i2'\n}\n\nfor idx in range(1, 41):\n    field_name_dict[f'genetic_pca{idx}'] = f'p22009_a{idx}'", "metadata": {"trusted": true, "tags": []}, "execution_count": 6, "outputs": []}, {"cell_type": "code", "source": "\nfield_names = list(field_name_dict.values())\n\n", "metadata": {"trusted": true, "tags": []}, "execution_count": 7, "outputs": []}, {"cell_type": "code", "source": "df = participant.retrieve_fields(names=field_names, engine=dxdata.connect(), coding_values=\"replace\")", "metadata": {"trusted": true, "tags": [], "collapsed": true, "jupyter": {"outputs_hidden": true}}, "execution_count": 8, "outputs": [{"name": "stdout", "text": "2023-10-03 21:05:58.839 WARN  ShellBasedUnixGroupsMapping:210 - unable to return groups for user Vx9Zg8q9PVYZ9PxjzkBK3bBZVJ36vQPq0yzKY3YQ__project-GQpgZf8JX0KKbFBGK9yff4Zg\nPartialGroupNameException The user name 'Vx9Zg8q9PVYZ9PxjzkBK3bBZVJ36vQPq0yzKY3YQ__project-GQpgZf8JX0KKbFBGK9yff4Zg' is not found. id: \u2018Vx9Zg8q9PVYZ9PxjzkBK3bBZVJ36vQPq0yzKY3YQ__project-GQpgZf8JX0KKbFBGK9yff4Zg\u2019: no such user\nid: \u2018Vx9Zg8q9PVYZ9PxjzkBK3bBZVJ36vQPq0yzKY3YQ__project-GQpgZf8JX0KKbFBGK9yff4Zg\u2019: no such user\n\n\tat org.apache.hadoop.security.ShellBasedUnixGroupsMapping.resolvePartialGroupNames(ShellBasedUnixGroupsMapping.java:294)\n\tat org.apache.hadoop.security.ShellBasedUnixGroupsMapping.getUnixGroups(ShellBasedUnixGroupsMapping.java:207)\n\tat org.apache.hadoop.security.ShellBasedUnixGroupsMapping.getGroups(ShellBasedUnixGroupsMapping.java:97)\n\tat org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.getGroups(JniBasedUnixGroupsMappingWithFallback.java:51)\n\tat org.apache.hadoop.security.Groups$GroupCacheLoader.fetchGroupList(Groups.java:387)\n\tat org.apache.hadoop.security.Groups$GroupCacheLoader.load(Groups.java:321)\n\tat org.apache.hadoop.security.Groups$GroupCacheLoader.load(Groups.java:270)\n\tat org.apache.hadoop.thirdparty.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3529)\n\tat org.apache.hadoop.thirdparty.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2278)\n\tat org.apache.hadoop.thirdparty.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2155)\n\tat org.apache.hadoop.thirdparty.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2045)\n\tat org.apache.hadoop.thirdparty.com.google.common.cache.LocalCache.get(LocalCache.java:3962)\n\tat org.apache.hadoop.thirdparty.com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3985)\n\tat org.apache.hadoop.thirdparty.com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4946)\n\tat org.apache.hadoop.security.Groups.getGroups(Groups.java:228)\n\tat org.apache.hadoop.security.UserGroupInformation.getGroups(UserGroupInformation.java:1734)\n\tat org.apache.hadoop.security.UserGroupInformation.getGroupNames(UserGroupInformation.java:1722)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:517)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:254)\n\tat org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:70)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1740)\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:83)\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:133)\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)\n\tat org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3650)\n\tat org.apache.hadoop.hive.ql.metadata.Hive.getMetaStoreClient(Hive.java:3696)\n\tat org.apache.hadoop.hive.ql.metadata.Hive.lambda$getMSC$0(Hive.java:3770)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3768)\n\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3682)\n\tat org.apache.hadoop.hive.ql.metadata.Hive.getDatabase(Hive.java:1600)\n\tat org.apache.hadoop.hive.ql.metadata.Hive.databaseExists(Hive.java:1588)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$databaseExists$1(HiveClientImpl.scala:396)\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:305)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:236)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:235)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:285)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.databaseExists(HiveClientImpl.scala:396)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$databaseExists$1(HiveExternalCatalog.scala:224)\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:102)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:224)\n\tat org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:150)\n\tat org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:140)\n\tat org.apache.spark.sql.internal.SharedState.globalTempViewManager$lzycompute(SharedState.scala:170)\n\tat org.apache.spark.sql.internal.SharedState.globalTempViewManager(SharedState.scala:168)\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.$anonfun$catalog$2(HiveSessionStateBuilder.scala:61)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.globalTempViewManager$lzycompute(SessionCatalog.scala:119)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.globalTempViewManager(SessionCatalog.scala:119)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.lookupGlobalTempView(SessionCatalog.scala:940)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveTempViews$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveTempViews$$lookupTempView(Analyzer.scala:950)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveTempViews$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveTempViews$$lookupAndResolveTempView(Analyzer.scala:963)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveTempViews$$anonfun$apply$12.applyOrElse(Analyzer.scala:901)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveTempViews$$anonfun$apply$12.applyOrElse(Analyzer.scala:899)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1122)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1121)\n\tat org.apache.spark.sql.catalyst.plans.logical.OrderPreservingUnaryNode.mapChildren(LogicalPlan.scala:206)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)\n\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1148)\n\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1147)\n\tat org.apache.spark.sql.catalyst.plans.logical.Join.mapChildren(basicLogicalOperators.scala:390)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)\n\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1148)\n\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1147)\n\tat org.apache.spark.sql.catalyst.plans.logical.Join.mapChildren(basicLogicalOperators.scala:390)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)\n\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1148)\n\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1147)\n\tat org.apache.spark.sql.catalyst.plans.logical.Join.mapChildren(basicLogicalOperators.scala:390)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)\n\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1148)\n\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1147)\n\tat org.apache.spark.sql.catalyst.plans.logical.Join.mapChildren(basicLogicalOperators.scala:390)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1122)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1121)\n\tat org.apache.spark.sql.catalyst.plans.logical.OrderPreservingUnaryNode.mapChildren(LogicalPlan.scala:206)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveTempViews$.apply(Analyzer.scala:899)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1164)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1131)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:211)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:208)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:200)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:200)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:222)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:218)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:167)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:218)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:182)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:179)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:179)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:203)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:202)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:75)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:183)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:183)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:73)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:65)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:618)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:613)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n", "output_type": "stream"}]}, {"cell_type": "code", "source": "# See the first five entries as a Pandas DataFrame:\ndf.limit(5).toPandas()\n", "metadata": {"trusted": true, "tags": []}, "execution_count": 9, "outputs": [{"name": "stdout", "text": "2023-10-03 21:06:10.259 WARN  package:69 - Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n", "output_type": "stream"}, {"name": "stderr", "text": "                                                                                \r", "output_type": "stream"}, {"execution_count": 9, "output_type": "execute_result", "data": {"text/plain": "       eid     p31  p21001_i0  p21001_i1  p21001_i2  \\\n0  3351884  Female    34.1036        NaN        NaN   \n1  4366755  Female    37.9464        NaN        NaN   \n2  1270557  Female    25.1205        NaN        NaN   \n3  3439885    Male    31.6320        NaN        NaN   \n4  4854010  Female    28.4083        NaN        NaN   \n\n                             p22021     p22006  p22001   p34  \\\n0                  No kinship found  Caucasian  Female  1961   \n1                  No kinship found  Caucasian  Female  1960   \n2                  No kinship found  Caucasian  Female  1943   \n3                  No kinship found  Caucasian    Male  1945   \n4  At least one relative identified  Caucasian  Female  1950   \n\n                  p2724_i0  ... p22009_a31 p22009_a32 p22009_a33 p22009_a34  \\\n0  Not sure - other reason  ...  -7.316860   1.283350    3.97141   1.290280   \n1                       No  ...   1.745990   0.585117    1.51662   0.994568   \n2                      Yes  ...  -1.936160   2.739070   -4.62403  -5.848920   \n3                     None  ...   2.886250   1.225440    2.22903   6.779000   \n4                      Yes  ...   0.493258   3.893070   -5.88067   0.579989   \n\n  p22009_a35  p22009_a36  p22009_a37  p22009_a38  p22009_a39  p22009_a40  \n0    1.06482     6.63408     3.03357   -1.999740    5.446890    -4.35153  \n1   -1.40280     3.86994     3.59829    1.325750   -0.374176     9.42301  \n2    3.94096     3.08413     1.04149   -0.374862   -2.457350    -3.94002  \n3    5.59474     3.49702    -1.54641    1.972960   -1.609130    -1.94949  \n4    2.76152    -1.07788     6.20570   -5.159830   -2.729040    -2.20650  \n\n[5 rows x 59 columns]", "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>eid</th>\n      <th>p31</th>\n      <th>p21001_i0</th>\n      <th>p21001_i1</th>\n      <th>p21001_i2</th>\n      <th>p22021</th>\n      <th>p22006</th>\n      <th>p22001</th>\n      <th>p34</th>\n      <th>p2724_i0</th>\n      <th>...</th>\n      <th>p22009_a31</th>\n      <th>p22009_a32</th>\n      <th>p22009_a33</th>\n      <th>p22009_a34</th>\n      <th>p22009_a35</th>\n      <th>p22009_a36</th>\n      <th>p22009_a37</th>\n      <th>p22009_a38</th>\n      <th>p22009_a39</th>\n      <th>p22009_a40</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3351884</td>\n      <td>Female</td>\n      <td>34.1036</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>No kinship found</td>\n      <td>Caucasian</td>\n      <td>Female</td>\n      <td>1961</td>\n      <td>Not sure - other reason</td>\n      <td>...</td>\n      <td>-7.316860</td>\n      <td>1.283350</td>\n      <td>3.97141</td>\n      <td>1.290280</td>\n      <td>1.06482</td>\n      <td>6.63408</td>\n      <td>3.03357</td>\n      <td>-1.999740</td>\n      <td>5.446890</td>\n      <td>-4.35153</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4366755</td>\n      <td>Female</td>\n      <td>37.9464</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>No kinship found</td>\n      <td>Caucasian</td>\n      <td>Female</td>\n      <td>1960</td>\n      <td>No</td>\n      <td>...</td>\n      <td>1.745990</td>\n      <td>0.585117</td>\n      <td>1.51662</td>\n      <td>0.994568</td>\n      <td>-1.40280</td>\n      <td>3.86994</td>\n      <td>3.59829</td>\n      <td>1.325750</td>\n      <td>-0.374176</td>\n      <td>9.42301</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1270557</td>\n      <td>Female</td>\n      <td>25.1205</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>No kinship found</td>\n      <td>Caucasian</td>\n      <td>Female</td>\n      <td>1943</td>\n      <td>Yes</td>\n      <td>...</td>\n      <td>-1.936160</td>\n      <td>2.739070</td>\n      <td>-4.62403</td>\n      <td>-5.848920</td>\n      <td>3.94096</td>\n      <td>3.08413</td>\n      <td>1.04149</td>\n      <td>-0.374862</td>\n      <td>-2.457350</td>\n      <td>-3.94002</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3439885</td>\n      <td>Male</td>\n      <td>31.6320</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>No kinship found</td>\n      <td>Caucasian</td>\n      <td>Male</td>\n      <td>1945</td>\n      <td>None</td>\n      <td>...</td>\n      <td>2.886250</td>\n      <td>1.225440</td>\n      <td>2.22903</td>\n      <td>6.779000</td>\n      <td>5.59474</td>\n      <td>3.49702</td>\n      <td>-1.54641</td>\n      <td>1.972960</td>\n      <td>-1.609130</td>\n      <td>-1.94949</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4854010</td>\n      <td>Female</td>\n      <td>28.4083</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>At least one relative identified</td>\n      <td>Caucasian</td>\n      <td>Female</td>\n      <td>1950</td>\n      <td>Yes</td>\n      <td>...</td>\n      <td>0.493258</td>\n      <td>3.893070</td>\n      <td>-5.88067</td>\n      <td>0.579989</td>\n      <td>2.76152</td>\n      <td>-1.07788</td>\n      <td>6.20570</td>\n      <td>-5.159830</td>\n      <td>-2.729040</td>\n      <td>-2.20650</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows \u00d7 59 columns</p>\n</div>"}, "metadata": {}}]}, {"cell_type": "code", "source": "def upload_file_to_project(filename, proj_dir):\n    dxpy.upload_local_file(filename, folder=proj_dir, parents=True)\n    print(f\"*********{filename} uploaded!!*********\")\n    return", "metadata": {"trusted": true, "tags": []}, "execution_count": 10, "outputs": []}, {"cell_type": "code", "source": "pandas_df = df.toPandas()", "metadata": {"trusted": true, "tags": []}, "execution_count": 11, "outputs": [{"name": "stderr", "text": "                                                                                \r", "output_type": "stream"}]}, {"cell_type": "code", "source": "pandas_df = pandas_df.rename(columns={v:k for k,v in field_name_dict.items()})", "metadata": {"trusted": true, "tags": []}, "execution_count": 12, "outputs": []}, {"cell_type": "code", "source": "start = 0\nfinal = len(pandas_df)\nblock = 0\nproj_dir = f\"/phenotype_processing/bmi_info/\"\nnrows = 50000\n\nfor i in range(10):\n    end = min(start+nrows, final)\n    table = pandas_df.iloc[start:end, :]\n    filename = f\"bmi_block{block}.csv.gz\"\n    table.to_csv(filename, index=False)\n    upload_file_to_project(filename, proj_dir)\n    start += nrows\n    block += 1\n    ", "metadata": {"trusted": true, "tags": []}, "execution_count": 13, "outputs": [{"name": "stdout", "text": "*********bmi_block0.csv.gz uploaded!!*********\n*********bmi_block1.csv.gz uploaded!!*********\n*********bmi_block2.csv.gz uploaded!!*********\n*********bmi_block3.csv.gz uploaded!!*********\n*********bmi_block4.csv.gz uploaded!!*********\n*********bmi_block5.csv.gz uploaded!!*********\n*********bmi_block6.csv.gz uploaded!!*********\n*********bmi_block7.csv.gz uploaded!!*********\n*********bmi_block8.csv.gz uploaded!!*********\n*********bmi_block9.csv.gz uploaded!!*********\n", "output_type": "stream"}]}, {"cell_type": "markdown", "source": "# Resources\n\n1. https://dnanexus.gitbook.io/uk-biobank-rap/working-on-the-research-analysis-platform/using-spark-to-analyze-tabular-data\n2. https://github.com/dnanexus/OpenBio/blob/master/UKB_notebooks/ukb-rap-pheno-basic.ipynb", "metadata": {}}, {"cell_type": "code", "source": "", "metadata": {}, "execution_count": null, "outputs": []}]}